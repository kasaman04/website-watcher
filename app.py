import asyncio
import json
import logging
import smtplib
import hashlib
import os
import time
import weakref
from datetime import datetime, timedelta
from email.mime.text import MIMEText
from email.mime.multipart import MIMEMultipart
from typing import List, Dict, Optional, Set
from logging.handlers import RotatingFileHandler
import httpx
from fastapi import FastAPI, HTTPException, Request
from fastapi.staticfiles import StaticFiles
from fastapi.responses import HTMLResponse, FileResponse
from fastapi.middleware.cors import CORSMiddleware
from slowapi import Limiter, _rate_limit_exceeded_handler
from slowapi.util import get_remote_address
from slowapi.errors import RateLimitExceeded
from pydantic import BaseModel, ValidationError
import uvicorn
from dotenv import load_dotenv

# ãƒ­ã‚°è¨­å®š - ãƒ­ãƒ¼ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ä»˜ã
log_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒ³ãƒ‰ãƒ© (æœ€å¤§10MBã€5ã¤ã¾ã§ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—)
file_handler = RotatingFileHandler(
    'watcher.log', 
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5,
    encoding='utf-8'
)
file_handler.setFormatter(log_formatter)

# ã‚³ãƒ³ã‚½ãƒ¼ãƒ«ãƒãƒ³ãƒ‰ãƒ©
console_handler = logging.StreamHandler()
console_handler.setFormatter(log_formatter)

# ãƒ«ãƒ¼ãƒˆãƒ­ã‚¬ãƒ¼è¨­å®š
logging.basicConfig(
    level=logging.INFO,
    handlers=[file_handler, console_handler]
)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°èª­ã¿è¾¼ã¿
load_dotenv()

# ãƒ¬ãƒ¼ãƒˆåˆ¶é™è¨­å®š
limiter = Limiter(key_func=get_remote_address)

app = FastAPI(title="Website Watcher", description="é«˜ä¿¡é ¼æ€§ã‚µã‚¤ãƒˆæ›´æ–°ç›£è¦–ã‚·ã‚¹ãƒ†ãƒ ")
app.state.limiter = limiter
app.add_exception_handler(RateLimitExceeded, _rate_limit_exceeded_handler)

# CORSè¨­å®šï¼ˆã‚¹ãƒãƒ›ã‚¢ã‚¯ã‚»ã‚¹å¯¾å¿œï¼‰
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # æœ¬ç•ªç’°å¢ƒã§ã¯å…·ä½“çš„ãªãƒ‰ãƒ¡ã‚¤ãƒ³ã‚’æŒ‡å®š
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã¨ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯
monitoring_task = None
sites_data = []
last_health_check = datetime.now()
failed_sites: Set[str] = set()  # ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ç”¨
site_failure_count: Dict[str, int] = {}  # å¤±æ•—å›æ•°è¿½è·¡
site_last_failure: Dict[str, datetime] = {}  # æœ€å¾Œã®å¤±æ•—æ™‚åˆ»
http_client: Optional[httpx.AsyncClient] = None  # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆ
api_cache: Dict[str, tuple] = {}  # APIã‚­ãƒ£ãƒƒã‚·ãƒ¥ (key: (data, timestamp))

class Site(BaseModel):
    url: str
    email: str
    name: Optional[str] = ""
    
    def __init__(self, **data):
        # URL validation
        if 'url' in data and not data['url'].startswith(('http://', 'https://')):
            data['url'] = 'https://' + data['url']
        super().__init__(**data)

class HealthStatus(BaseModel):
    status: str
    timestamp: datetime
    monitoring_active: bool
    total_sites: int
    failed_sites: int
    last_check: Optional[datetime] = None
    uptime: str

class EmailService:
    """é«˜ä¿¡é ¼æ€§ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã‚µãƒ¼ãƒ“ã‚¹ - ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä»˜ã"""
    
    def __init__(self):
        self.smtp_server = os.getenv("SMTP_SERVER", "smtp.gmail.com")
        self.smtp_port = int(os.getenv("SMTP_PORT", "587"))
        self.username = os.getenv("SMTP_USERNAME", "")
        self.password = os.getenv("SMTP_PASSWORD", "")
        self.from_email = os.getenv("FROM_EMAIL", "")
        self.failure_count = 0
        self.last_failure_time = None
        self.circuit_breaker_threshold = 5  # 5å›é€£ç¶šå¤±æ•—ã§ã‚µãƒ¼ã‚­ãƒƒãƒˆã‚ªãƒ¼ãƒ—ãƒ³
        self.circuit_breaker_timeout = 300  # 5åˆ†ã§ãƒªã‚»ãƒƒãƒˆ
        
        # è¨­å®šé¨“è¨¼
        self._validate_config()
    
    def _validate_config(self):
        """ãƒ¡ãƒ¼ãƒ«è¨­å®šã®éªŒè¨¼"""
        missing = []
        if not self.username: missing.append("SMTP_USERNAME")
        if not self.password: missing.append("SMTP_PASSWORD")
        if not self.from_email: missing.append("FROM_EMAIL")
        
        if missing:
            logger.error(f"å¿…è¦ãªç’°å¢ƒå¤‰æ•°ãŒæœªè¨­å®š: {', '.join(missing)}")
            raise ValueError(f"Missing required environment variables: {', '.join(missing)}")
    
    def _is_circuit_open(self) -> bool:
        """ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯"""
        if self.failure_count < self.circuit_breaker_threshold:
            return False
            
        if self.last_failure_time is None:
            return False
            
        # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¾Œã¯ãƒªã‚»ãƒƒãƒˆ
        if time.time() - self.last_failure_time > self.circuit_breaker_timeout:
            self.failure_count = 0
            self.last_failure_time = None
            logger.info("ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ")
            return False
            
        return True
    
    def _exponential_backoff(self, attempt: int) -> float:
        """æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•è¨ˆç®—"""
        return min(2 ** attempt, 60)  # æœ€å¤§60ç§’
    
    async def send_email(self, to_email: str, subject: str, body: str) -> bool:
        """éåŒæœŸãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä»˜ãï¼‰"""
        if self._is_circuit_open():
            logger.warning(f"ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã‚ªãƒ¼ãƒ—ãƒ³ä¸­ - é€ä¿¡ã‚¹ã‚­ãƒƒãƒ—: {to_email}")
            return False
        
        max_retries = 3
        for attempt in range(max_retries):
            try:
                logger.info(f"ãƒ¡ãƒ¼ãƒ«é€ä¿¡è©¦è¡Œ {attempt + 1}/{max_retries}: {to_email}")
                
                # ãƒ¡ãƒ¼ãƒ«ä½œæˆ
                msg = MIMEMultipart()
                msg['From'] = self.from_email
                msg['To'] = to_email
                msg['Subject'] = subject
                msg.attach(MIMEText(body, 'plain', 'utf-8'))
                
                # éåŒæœŸSMTPé€ä¿¡
                def _send_sync():
                    with smtplib.SMTP(self.smtp_server, self.smtp_port, timeout=30) as server:
                        server.starttls()
                        server.login(self.username, self.password)
                        server.send_message(msg)
                
                # ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°å‡¦ç†ã‚’éåŒæœŸå®Ÿè¡Œ
                await asyncio.to_thread(_send_sync)
                
                logger.info(f"âœ… ãƒ¡ãƒ¼ãƒ«é€ä¿¡æˆåŠŸ: {to_email}")
                self.failure_count = 0  # æˆåŠŸæ™‚ã¯ãƒªã‚»ãƒƒãƒˆ
                return True
                
            except Exception as e:
                logger.error(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡å¤±æ•— (è©¦è¡Œ{attempt + 1}): {e}")
                if attempt < max_retries - 1:
                    backoff_time = self._exponential_backoff(attempt)
                    logger.info(f"âš™ï¸ {backoff_time}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤")
                    await asyncio.sleep(backoff_time)
        
        # å…¨ã¦ã®è©¦è¡ŒãŒå¤±æ•—
        self.failure_count += 1
        self.last_failure_time = time.time()
        logger.error(f"âŒ ãƒ¡ãƒ¼ãƒ«é€ä¿¡å®Œå…¨å¤±æ•—: {to_email} (å¤±æ•—å›æ•°: {self.failure_count})")
        return False
    
    async def test_connection(self) -> bool:
        """éåŒæœŸSMTPæ¥ç¶šãƒ†ã‚¹ãƒˆ"""
        try:
            logger.info("SMTPæ¥ç¶šãƒ†ã‚¹ãƒˆé–‹å§‹...")
            
            def _test_sync():
                with smtplib.SMTP(self.smtp_server, self.smtp_port, timeout=10) as server:
                    server.starttls()
                    server.login(self.username, self.password)
            
            await asyncio.to_thread(_test_sync)
            logger.info("âœ… SMTPæ¥ç¶šæˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"âŒ SMTPæ¥ç¶šå¤±æ•—: {e}")
            return False

class SiteChecker:
    """éåŒæœŸã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ã‚¯ãƒ©ã‚¹ - ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ä»˜ã"""
    
    def __init__(self, http_client: httpx.AsyncClient):
        self.http_client = http_client
        self.site_timeouts = {}  # ã‚µã‚¤ãƒˆåˆ¥ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š
    
    def _get_timeout_for_site(self, url: str) -> int:
        """ã‚µã‚¤ãƒˆåˆ¥ã®å‹•çš„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå–å¾—"""
        base_timeout = int(os.getenv("SITE_TIMEOUT", "15"))
        return self.site_timeouts.get(url, base_timeout)
    
    def _update_timeout_for_site(self, url: str, response_time: float):
        """ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã«åŸºã¥ã„ã¦ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’èª¿æ•´"""
        # ãƒ¬ã‚¹ãƒãƒ³ã‚¹æ™‚é–“ã®1.5å€ã‚’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã¨ã—ã¦è¨­å®šï¼ˆæœ€å¤§60ç§’ï¼‰
        new_timeout = min(max(int(response_time * 1.5), 10), 60)
        self.site_timeouts[url] = new_timeout
    
    def _is_site_in_circuit_breaker(self, url: str) -> bool:
        """ã‚µã‚¤ãƒˆãŒã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼çŠ¶æ…‹ã‹ãƒã‚§ãƒƒã‚¯"""
        if url not in site_failure_count:
            return False
            
        failure_count = site_failure_count[url]
        if failure_count < 3:  # 3å›é€£ç¶šå¤±æ•—ã§ã‚µãƒ¼ã‚­ãƒƒãƒˆã‚ªãƒ¼ãƒ—ãƒ³
            return False
            
        last_failure = site_last_failure.get(url)
        if last_failure is None:
            return False
            
        # 10åˆ†çµŒéã§ãƒªã‚»ãƒƒãƒˆ
        if datetime.now() - last_failure > timedelta(minutes=10):
            site_failure_count[url] = 0
            if url in site_last_failure:
                del site_last_failure[url]
            if url in failed_sites:
                failed_sites.remove(url)
            logger.info(f"ã‚µã‚¤ãƒˆã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ãƒªã‚»ãƒƒãƒˆ: {url}")
            return False
            
        return True
    
    async def get_site_hash(self, url: str) -> Optional[str]:
        """éåŒæœŸã‚µã‚¤ãƒˆãƒãƒƒã‚·ãƒ¥å–å¾—"""
        if self._is_site_in_circuit_breaker(url):
            logger.warning(f"ã‚µã‚¤ãƒˆã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã‚ªãƒ¼ãƒ—ãƒ³ä¸­ - ã‚¹ã‚­ãƒƒãƒ—: {url}")
            return None
        
        try:
            logger.info(f"ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯é–‹å§‹: {url}")
            start_time = time.time()
            
            timeout_val = self._get_timeout_for_site(url)
            headers = {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
                'Accept-Language': 'ja,en-US;q=0.5',
                'Accept-Encoding': 'gzip, deflate',
                'Connection': 'keep-alive',
                'Upgrade-Insecure-Requests': '1'
            }
            
            response = await self.http_client.get(
                url, 
                timeout=timeout_val, 
                headers=headers,
                follow_redirects=True
            )
            response.raise_for_status()
            
            response_time = time.time() - start_time
            self._update_timeout_for_site(url, response_time)
            
            # ãƒãƒƒã‚·ãƒ¥è¨ˆç®—
            content_hash = hashlib.md5(response.text.encode('utf-8')).hexdigest()
            
            # æˆåŠŸæ™‚ã¯å¤±æ•—ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ã‚’ãƒªã‚»ãƒƒãƒˆ
            if url in site_failure_count:
                site_failure_count[url] = 0
            if url in failed_sites:
                failed_sites.remove(url)
            
            logger.info(f"âœ… ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯æˆåŠŸ: {url} (hash: {content_hash[:8]}..., {response_time:.2f}s)")
            return content_hash
            
        except Exception as e:
            # å¤±æ•—æƒ…å ±ã‚’è¨˜éŒ²
            site_failure_count[url] = site_failure_count.get(url, 0) + 1
            site_last_failure[url] = datetime.now()
            failed_sites.add(url)
            
            logger.error(f"âŒ ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯å¤±æ•—: {url} - {e} (å¤±æ•—å›æ•°: {site_failure_count[url]})")
            return None

# ã‚µãƒ¼ãƒ“ã‚¹ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ (é…å»¶åˆæœŸåŒ–)
email_service = None
site_checker = None

# ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³èµ·å‹•æ™‚åˆ»
app_start_time = datetime.now()

def load_sites() -> List[Dict]:
    """å …ç‰¢ãªã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—å¯¾å¿œï¼‰"""
    config_files = ['config.json', 'config.json.backup']
    
    for config_file in config_files:
        try:
            if os.path.exists(config_file):
                with open(config_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    sites = data.get('sites', [])
                    logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿æˆåŠŸ: {config_file} ({len(sites)}ã‚µã‚¤ãƒˆ)")
                    return sites
        except (json.JSONDecodeError, IOError) as e:
            logger.warning(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•—: {config_file} - {e}")
            continue
    
    logger.warning("ã™ã¹ã¦ã®è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®èª­ã¿è¾¼ã¿ã«å¤±æ•—")
    return []

def save_sites(sites: List[Dict]):
    """ã‚¢ãƒˆãƒŸãƒƒã‚¯ãªã‚µã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ä¿å­˜ï¼ˆãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä»˜ãï¼‰"""
    try:
        # ç¾åœ¨ã®è¨­å®šã‚’ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—
        if os.path.exists('config.json'):
            import shutil
            shutil.copy2('config.json', 'config.json.backup')
        
        # æ–°ã—ã„è¨­å®šã‚’ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãè¾¼ã¿
        config = {
            'sites': sites, 
            'settings': {
                'check_interval': int(os.getenv('CHECK_INTERVAL', '300')),
                'timeout': int(os.getenv('SITE_TIMEOUT', '15')),
                'max_retries': 3
            },
            'last_updated': datetime.now().isoformat()
        }
        
        temp_file = 'config.json.tmp'
        with open(temp_file, 'w', encoding='utf-8') as f:
            json.dump(config, f, indent=2, ensure_ascii=False)
        
        # ã‚¢ãƒˆãƒŸãƒƒã‚¯ã«ç½®ãæ›ãˆ
        import os
        if os.name == 'nt':  # Windows
            if os.path.exists('config.json'):
                os.remove('config.json')
        os.rename(temp_file, 'config.json')
        
        logger.info(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜å®Œäº† ({len(sites)}ã‚µã‚¤ãƒˆ)")
        
    except Exception as e:
        logger.error(f"è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
        # ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ãŒæ®‹ã£ã¦ã„ãŸã‚‰å‰Šé™¤
        if os.path.exists('config.json.tmp'):
            try:
                os.remove('config.json.tmp')
            except:
                pass

async def check_all_sites():
    """å…¨ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯ï¼ˆä¸¦åˆ—å‡¦ç†ãƒ»ã‚¨ãƒ©ãƒ¼å‡¦ç†å¼·åŒ–ç‰ˆï¼‰"""
    global sites_data, last_health_check
    
    try:
        sites_data = load_sites()
        if not sites_data:
            logger.info("ç›£è¦–å¯¾è±¡ã‚µã‚¤ãƒˆãŒã‚ã‚Šã¾ã›ã‚“")
            return
        
        logger.info(f"ğŸ” {len(sites_data)}ã‚µã‚¤ãƒˆã®ä¸¦åˆ—ç›£è¦–ã‚’é–‹å§‹")
        last_health_check = datetime.now()
        
        # ä¸¦åˆ—å‡¦ç†ã§ã‚µã‚¤ãƒˆã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆæœ€å¤§5ä¸¦åˆ—ï¼‰
        semaphore = asyncio.Semaphore(5)
        tasks = []
        
        for site in sites_data:
            task = asyncio.create_task(_check_single_site(site, semaphore))
            tasks.append(task)
        
        # å…¨ã¦ã®ã‚¿ã‚¹ã‚¯ãŒå®Œäº†ã™ã‚‹ã¾ã§å¾…æ©Ÿ
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # çµæœã‚’å‡¦ç†
        success_count = sum(1 for r in results if r is True)
        error_count = sum(1 for r in results if isinstance(r, Exception))
        skip_count = len(results) - success_count - error_count
        
        logger.info(f"ğŸ“Š ãƒã‚§ãƒƒã‚¯å®Œäº†: æˆåŠŸ={success_count}, ã‚¨ãƒ©ãƒ¼={error_count}, ã‚¹ã‚­ãƒƒãƒ—={skip_count}")
        
        # è¨­å®šä¿å­˜ï¼ˆã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¦ã‚‚ãƒ‡ãƒ¼ã‚¿ã¯ä¿å­˜ï¼‰
        save_sites(sites_data)
        
    except Exception as e:
        logger.error(f"ç›£è¦–å‡¦ç†ã§äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")

async def _check_single_site(site: Dict, semaphore: asyncio.Semaphore) -> bool:
    """å˜ä¸€ã‚µã‚¤ãƒˆã®éåŒæœŸãƒã‚§ãƒƒã‚¯"""
    async with semaphore:
        try:
            url = site['url']
            email = site['email']
            name = site.get('name', url)
            last_hash = site.get('hash', '')
            
            # ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚¯
            current_hash = await site_checker.get_site_hash(url)
            if not current_hash:
                return False  # ã‚µãƒ¼ã‚­ãƒƒãƒˆãƒ–ãƒ¬ãƒ¼ã‚«ãƒ¼ã¾ãŸã¯ã‚¨ãƒ©ãƒ¼
            
            # åˆå›ãƒã‚§ãƒƒã‚¯
            if not last_hash:
                site['hash'] = current_hash
                site['last_check'] = datetime.now().isoformat()
                logger.info(f"ğŸ“ åˆå›ãƒãƒƒã‚·ãƒ¥è¨­å®š: {name}")
                return True
            
            # å¤‰æ›´æ¤œçŸ¥
            if current_hash != last_hash:
                logger.info(f"ğŸš¨ å¤‰æ›´æ¤œçŸ¥: {name}")
                
                # ãƒ¡ãƒ¼ãƒ«é€ä¿¡
                subject = f"ğŸ”” ã‚µã‚¤ãƒˆæ›´æ–°é€šçŸ¥: {name}"
                body = f"""ã‚µã‚¤ãƒˆãŒæ›´æ–°ã•ã‚Œã¾ã—ãŸï¼

ã‚µã‚¤ãƒˆå: {name}
URL: {url}
æ›´æ–°æ¤œçŸ¥æ™‚åˆ»: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}

ã“ã®ãƒ¡ãƒ¼ãƒ«ã¯ Website Watcher ã«ã‚ˆã‚Šè‡ªå‹•é€ä¿¡ã•ã‚Œã¾ã—ãŸã€‚
é‹ç”¨çŠ¶æ³: https://your-domain:8888/api/health
"""
                
                success = await email_service.send_email(email, subject, body)
                if success:
                    # ãƒãƒƒã‚·ãƒ¥æ›´æ–°
                    site['hash'] = current_hash
                    site['last_check'] = datetime.now().isoformat()
                    site['last_notified'] = datetime.now().isoformat()
                    logger.info(f"âœ… é€šçŸ¥å®Œäº†: {name} â†’ {email}")
                else:
                    logger.error(f"âŒ é€šçŸ¥å¤±æ•—: {name} â†’ {email}")
                    # é€šçŸ¥å¤±æ•—ã§ã‚‚last_checkã¯æ›´æ–°ï¼ˆç„¡é™ãƒªãƒˆãƒ©ã‚¤ã‚’é˜²ãï¼‰
                    site['last_check'] = datetime.now().isoformat()
            else:
                site['last_check'] = datetime.now().isoformat()
                logger.info(f"ğŸ“ å¤‰æ›´ãªã—: {name}")
            
            # ã‚µã‚¤ãƒˆé–“ã®ãƒ¬ãƒ¼ãƒˆåˆ¶é™
            await asyncio.sleep(1)
            return True
            
        except Exception as e:
            logger.error(f"ã‚µã‚¤ãƒˆå€‹åˆ¥ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼ ({site.get('name', site.get('url', 'unknown'))}): {e}")
            return False

async def monitoring_loop():
    """å …ç‰¢ãªç›£è¦–ãƒ«ãƒ¼ãƒ—ï¼ˆè‡ªå‹•å¾©æ—§ãƒ»æŒ‡æ•°ãƒãƒƒã‚¯ã‚ªãƒ•ä»˜ãï¼‰"""
    logger.info("ğŸ”„ ç›£è¦–ãƒ«ãƒ¼ãƒ—é–‹å§‹")
    consecutive_errors = 0
    max_errors = 10
    
    while True:
        try:
            await check_all_sites()
            consecutive_errors = 0  # æˆåŠŸæ™‚ã¯ã‚¨ãƒ©ãƒ¼ã‚«ã‚¦ãƒ³ã‚¿ãƒ¼ãƒªã‚»ãƒƒãƒˆ
            
            check_interval = max(int(os.getenv("CHECK_INTERVAL", "300")), 60)  # æœ€å°60ç§’
            logger.info(f"â° {check_interval}ç§’å¾Œã«å†ãƒã‚§ãƒƒã‚¯")
            await asyncio.sleep(check_interval)
            
        except asyncio.CancelledError:
            logger.info("ç›£è¦–ãƒ«ãƒ¼ãƒ—ãŒã‚­ãƒ£ãƒ³ã‚»ãƒ«ã•ã‚Œã¾ã—ãŸ")
            break
            
        except Exception as e:
            consecutive_errors += 1
            backoff_time = min(2 ** consecutive_errors, 300)  # æœ€å¤§5åˆ†
            
            logger.error(f"ç›£è¦–ãƒ«ãƒ¼ãƒ—ã‚¨ãƒ©ãƒ¼ ({consecutive_errors}/{max_errors}): {e}")
            logger.info(f"âš™ï¸ {backoff_time}ç§’å¾…æ©Ÿå¾Œã«ãƒªãƒˆãƒ©ã‚¤")
            
            # é€£ç¶šã‚¨ãƒ©ãƒ¼ãŒå¤šã™ãã‚‹å ´åˆã¯é•·æ™‚é–“å¾…æ©Ÿ
            if consecutive_errors >= max_errors:
                logger.critical(f"é€£ç¶šã‚¨ãƒ©ãƒ¼ä¸Šé™åˆ°é” - 10åˆ†é–“å¾…æ©Ÿ")
                consecutive_errors = 0  # ãƒªã‚»ãƒƒãƒˆ
                await asyncio.sleep(600)  # 10åˆ†
            else:
                await asyncio.sleep(backoff_time)

async def task_monitor():
    """ç›£è¦–ã‚¿ã‚¹ã‚¯ã®å¥å…¨æ€§ã‚’ãƒã‚§ãƒƒã‚¯ã—ã€å¿…è¦ã«å¿œã˜ã¦å†èµ·å‹•"""
    global monitoring_task
    
    while True:
        try:
            await asyncio.sleep(300)  # 5åˆ†ã”ã¨ã«ãƒã‚§ãƒƒã‚¯
            
            if monitoring_task is None or monitoring_task.done():
                if monitoring_task and monitoring_task.done():
                    exception = monitoring_task.exception()
                    if exception:
                        logger.error(f"ç›£è¦–ã‚¿ã‚¹ã‚¯ãŒç•°å¸¸çµ‚äº†: {exception}")
                    else:
                        logger.warning("ç›£è¦–ã‚¿ã‚¹ã‚¯ãŒæ­£å¸¸çµ‚äº†ï¼ˆäºˆæœŸã—ãªã„ï¼‰")
                
                logger.info("ğŸ”„ ç›£è¦–ã‚¿ã‚¹ã‚¯ã‚’å†èµ·å‹•")
                monitoring_task = asyncio.create_task(monitoring_loop())
                
        except asyncio.CancelledError:
            break
        except Exception as e:
            logger.error(f"ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‹ã‚¿ãƒ¼ã‚¨ãƒ©ãƒ¼: {e}")
            await asyncio.sleep(60)

# é™çš„ãƒ•ã‚¡ã‚¤ãƒ«é…ä¿¡
import os
app_dir = os.path.dirname(os.path.abspath(__file__))
static_dir = os.path.join(app_dir, "static")
app.mount("/static", StaticFiles(directory=static_dir), name="static")

# FastAPI ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.on_event("startup")
async def startup_event():
    """ã‚¢ãƒ—ãƒªèµ·å‹•æ™‚ã®å‡¦ç†ï¼ˆã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–ãƒ»ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
    global monitoring_task, email_service, site_checker, http_client
    
    logger.info("ğŸš€ Website Watcher èµ·å‹•")
    
    try:
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        http_client = httpx.AsyncClient(
            timeout=30.0,
            limits=httpx.Limits(max_keepalive_connections=20, max_connections=100),
            verify=True,
            follow_redirects=True
        )
        logger.info("âœ… HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        
        # ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–
        try:
            email_service = EmailService()
            logger.info("âœ… ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å®Œäº†")
        except ValueError as e:
            logger.error(f"âŒ ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹åˆæœŸåŒ–å¤±æ•—: {e}")
            email_service = None  # ãƒ¡ãƒ¼ãƒ«æ©Ÿèƒ½ç„¡åŠ¹åŒ–
        
        site_checker = SiteChecker(http_client)
        logger.info("âœ… ã‚µã‚¤ãƒˆãƒã‚§ãƒƒã‚«ãƒ¼åˆæœŸåŒ–å®Œäº†")
        
        # SMTPæ¥ç¶šãƒ†ã‚¹ãƒˆï¼ˆãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ãŒæœ‰åŠ¹ãªå ´åˆã®ã¿ï¼‰
        if email_service:
            if await email_service.test_connection():
                logger.info("âœ… ãƒ¡ãƒ¼ãƒ«è¨­å®šç¢ºèªå®Œäº†")
            else:
                logger.warning("âš ï¸ ãƒ¡ãƒ¼ãƒ«è¨­å®šã«å•é¡ŒãŒã‚ã‚Šã¾ã™ - é€šçŸ¥ã¯ç„¡åŠ¹åŒ–ã•ã‚Œã¾ã™")
        
        # ç›£è¦–ã‚¿ã‚¹ã‚¯é–‹å§‹
        monitoring_task = asyncio.create_task(monitoring_loop())
        
        # ã‚¿ã‚¹ã‚¯ãƒ¢ãƒ‹ã‚¿ãƒ¼é–‹å§‹ï¼ˆç›£è¦–ã‚¿ã‚¹ã‚¯ã®æ­»æ´»ç›£è¦–ï¼‰
        task_monitor_task = asyncio.create_task(task_monitor())
        
        logger.info("ğŸ¯ å…¨ã‚µãƒ¼ãƒ“ã‚¹èµ·å‹•å®Œäº†")
        
    except Exception as e:
        logger.critical(f"âŒ èµ·å‹•æ™‚ã‚¨ãƒ©ãƒ¼: {e}")
        raise

@app.on_event("shutdown")
async def shutdown_event():
    """ã‚¢ãƒ—ãƒªçµ‚äº†æ™‚ã®å‡¦ç†ï¼ˆãƒªã‚½ãƒ¼ã‚¹è§£æ”¾ï¼‰"""
    global monitoring_task, http_client
    
    logger.info("ğŸ›‘ Website Watcher çµ‚äº†å‡¦ç†é–‹å§‹")
    
    try:
        # ç›£è¦–ã‚¿ã‚¹ã‚¯çµ‚äº†
        if monitoring_task and not monitoring_task.done():
            monitoring_task.cancel()
            try:
                await asyncio.wait_for(monitoring_task, timeout=10.0)
            except (asyncio.CancelledError, asyncio.TimeoutError):
                pass
        
        # HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ‚äº†
        if http_client:
            await http_client.aclose()
            logger.info("âœ… HTTPã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆçµ‚äº†")
        
        logger.info("ğŸ‘‹ Website Watcher çµ‚äº†å®Œäº†")
        
    except Exception as e:
        logger.error(f"çµ‚äº†æ™‚ã‚¨ãƒ©ãƒ¼: {e}")

@app.get("/", response_class=HTMLResponse)
async def root():
    """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸"""
    return FileResponse(os.path.join(static_dir, "index.html"))

@app.get("/api/sites")
@limiter.limit("10/minute")
async def get_sites(request: Request):
    """ã‚µã‚¤ãƒˆä¸€è¦§å–å¾—ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãï¼‰"""
    cache_key = "sites_list"
    now = time.time()
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ï¼ˆ30ç§’æœ‰åŠ¹ï¼‰
    if cache_key in api_cache:
        data, timestamp = api_cache[cache_key]
        if now - timestamp < 30:
            return data
    
    try:
        sites = load_sites()
        result = {"sites": sites, "cached": False, "timestamp": datetime.now().isoformat()}
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥æ›´æ–°
        api_cache[cache_key] = (result, now)
        
        return result
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆä¸€è¦§å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ã‚µã‚¤ãƒˆä¸€è¦§ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

@app.post("/api/sites")
@limiter.limit("5/minute")
async def add_site(request: Request, site: Site):
    """ã‚µã‚¤ãƒˆè¿½åŠ ï¼ˆå…¥åŠ›æ¤œè¨¼å¼·åŒ–ï¼‰"""
    try:
        sites = load_sites()
        
        # URLã®æ­£è¦åŒ–ã¨ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        normalized_url = site.url.strip()
        if not normalized_url.startswith(('http://', 'https://')):
            normalized_url = 'https://' + normalized_url
        
        # é‡è¤‡ãƒã‚§ãƒƒã‚¯
        for existing_site in sites:
            if existing_site['url'] == normalized_url:
                raise HTTPException(status_code=400, detail="ã“ã®URLã¯æ—¢ã«ç™»éŒ²ã•ã‚Œã¦ã„ã¾ã™")
        
        # ã‚µã‚¤ãƒˆæ•°åˆ¶é™
        max_sites = int(os.getenv('MAX_SITES_PER_INSTANCE', '50'))
        if len(sites) >= max_sites:
            raise HTTPException(status_code=400, detail=f"ã‚µã‚¤ãƒˆç™»éŒ²æ•°ä¸Šé™({max_sites})ã«é”ã—ã¾ã—ãŸ")
        
        # æ–°ã‚µã‚¤ãƒˆè¿½åŠ 
        new_site = {
            "url": normalized_url,
            "email": site.email.strip().lower(),
            "name": site.name.strip() if site.name else normalized_url,
            "hash": "",
            "created_at": datetime.now().isoformat()
        }
        
        sites.append(new_site)
        save_sites(sites)
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if "sites_list" in api_cache:
            del api_cache["sites_list"]
        
        logger.info(f"ğŸ“ æ–°ã‚µã‚¤ãƒˆç™»éŒ²: {new_site['name']} ({new_site['url']})")
        return {"message": "ã‚µã‚¤ãƒˆã‚’ç™»éŒ²ã—ã¾ã—ãŸ", "site": new_site}
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆè¿½åŠ ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ã‚µã‚¤ãƒˆã®è¿½åŠ ã«å¤±æ•—ã—ã¾ã—ãŸ")

@app.delete("/api/sites/{site_index}")
@limiter.limit("5/minute")
async def delete_site(request: Request, site_index: int):
    """ã‚µã‚¤ãƒˆå‰Šé™¤ï¼ˆå®‰å…¨æ€§å¼·åŒ–ï¼‰"""
    try:
        sites = load_sites()
        
        if 0 <= site_index < len(sites):
            deleted_site = sites.pop(site_index)
            save_sites(sites)
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
            if "sites_list" in api_cache:
                del api_cache["sites_list"]
            
            logger.info(f"ğŸ—‘ï¸ ã‚µã‚¤ãƒˆå‰Šé™¤: {deleted_site.get('name', deleted_site['url'])}")
            return {"message": "ã‚µã‚¤ãƒˆã‚’å‰Šé™¤ã—ã¾ã—ãŸ", "deleted_site": deleted_site}
        else:
            raise HTTPException(status_code=404, detail="ã‚µã‚¤ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"ã‚µã‚¤ãƒˆå‰Šé™¤ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ã‚µã‚¤ãƒˆã®å‰Šé™¤ã«å¤±æ•—ã—ã¾ã—ãŸ")

@app.post("/api/test-email")
@limiter.limit("3/minute")
async def test_email(request: Request, email_data: dict):
    """ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«é€ä¿¡ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™ä»˜ãï¼‰"""
    if not email_service:
        raise HTTPException(status_code=503, detail="ãƒ¡ãƒ¼ãƒ«ã‚µãƒ¼ãƒ“ã‚¹ãŒç„¡åŠ¹ã§ã™")
    
    to_email = email_data.get("email")
    if not to_email:
        raise HTTPException(status_code=400, detail="ãƒ¡ãƒ¼ãƒ«ã‚¢ãƒ‰ãƒ¬ã‚¹ãŒå¿…è¦ã§ã™")
    
    try:
        subject = "ğŸ“§ Website Watcher ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«"
        body = f"""ã“ã®ãƒ¡ãƒ¼ãƒ«ã¯ Website Watcher ã®ãƒ†ã‚¹ãƒˆé€ä¿¡ã§ã™ã€‚

é€ä¿¡æ™‚åˆ»: {datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')}
ã‚µãƒ¼ãƒãƒ¼çŠ¶æ…‹: æ­£å¸¸å‹•ä½œä¸­

ã“ã®ãƒ¡ãƒ¼ãƒ«ãŒå±Šã„ã¦ã„ã‚Œã°ã€ãƒ¡ãƒ¼ãƒ«è¨­å®šã¯æ­£å¸¸ã§ã™ã€‚
é‹ç”¨çŠ¶æ³: http://localhost:8888/api/health
"""
        
        success = await email_service.send_email(to_email, subject, body)
        if success:
            return {"message": "ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«ã‚’é€ä¿¡ã—ã¾ã—ãŸ"}
        else:
            raise HTTPException(status_code=500, detail="ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")
    except Exception as e:
        logger.error(f"ãƒ†ã‚¹ãƒˆãƒ¡ãƒ¼ãƒ«ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒ¡ãƒ¼ãƒ«é€ä¿¡ã«å¤±æ•—ã—ã¾ã—ãŸ")

@app.post("/api/check-now")
@limiter.limit("2/minute")
async def check_now(request: Request):
    """æ‰‹å‹•ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œï¼ˆé©åº¦ãªåˆ¶é™ä»˜ãï¼‰"""
    try:
        logger.info("ğŸ” æ‰‹å‹•ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ")
        start_time = time.time()
        
        await check_all_sites()
        
        end_time = time.time()
        duration = end_time - start_time
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¯ãƒªã‚¢
        if "sites_list" in api_cache:
            del api_cache["sites_list"]
        
        return {
            "message": "ãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¾ã—ãŸ",
            "duration": f"{duration:.2f}ç§’",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        logger.error(f"æ‰‹å‹•ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒã‚§ãƒƒã‚¯ã®å®Ÿè¡Œã«å¤±æ•—ã—ã¾ã—ãŸ")

# ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆ
@app.get("/api/health")
async def health_check():
    """ã‚¢ãƒ—ãƒªã‚±ãƒ¼ã‚·ãƒ§ãƒ³ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯"""
    try:
        current_time = datetime.now()
        uptime = current_time - app_start_time
        
        # ã‚µãƒ¼ãƒ“ã‚¹çŠ¶æ…‹ãƒã‚§ãƒƒã‚¯
        monitoring_active = monitoring_task is not None and not monitoring_task.done()
        email_available = email_service is not None
        sites = load_sites()
        
        health_data = {
            "status": "healthy" if monitoring_active else "degraded",
            "timestamp": current_time.isoformat(),
            "uptime": str(uptime),
            "uptime_seconds": uptime.total_seconds(),
            "monitoring_active": monitoring_active,
            "email_service_available": email_available,
            "total_sites": len(sites),
            "failed_sites": len(failed_sites),
            "last_health_check": last_health_check.isoformat() if last_health_check else None,
            "circuit_breakers": {
                "email_failures": email_service.failure_count if email_service else 0,
                "site_failures": dict(site_failure_count)
            },
            "memory_info": {
                "cache_entries": len(api_cache)
            }
        }
        
        return health_data
        
    except Exception as e:
        logger.error(f"ãƒ˜ãƒ«ã‚¹ãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
        return {
            "status": "error",
            "timestamp": datetime.now().isoformat(),
            "error": str(e)
        }

@app.get("/api/metrics")
async def metrics():
    """ãƒ¡ãƒˆãƒªã‚¯ã‚¹æƒ…å ±ï¼ˆãƒ¢ãƒ‹ã‚¿ãƒªãƒ³ã‚°ç”¨ï¼‰"""
    try:
        sites = load_sites()
        current_time = datetime.now()
        
        # æœ€è¿‘ã®ãƒã‚§ãƒƒã‚¯æ™‚åˆ»ã‚’è¨ˆç®—
        last_checks = []
        for site in sites:
            if 'last_check' in site and site['last_check']:
                try:
                    last_check = datetime.fromisoformat(site['last_check'])
                    last_checks.append(last_check)
                except:
                    pass
        
        metrics_data = {
            "total_sites": len(sites),
            "active_sites": len([s for s in sites if 'hash' in s and s['hash']]),
            "failed_sites_count": len(failed_sites),
            "email_circuit_breaker_open": email_service._is_circuit_open() if email_service else False,
            "site_timeouts": dict(site_checker.site_timeouts) if site_checker else {},
            "last_successful_check": max(last_checks).isoformat() if last_checks else None,
            "monitoring_uptime_seconds": (current_time - app_start_time).total_seconds(),
            "cache_hit_ratio": len(api_cache) / max(len(sites), 1)
        }
        
        return metrics_data
        
    except Exception as e:
        logger.error(f"ãƒ¡ãƒˆãƒªã‚¯ã‚¹å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
        raise HTTPException(status_code=500, detail="ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã®å–å¾—ã«å¤±æ•—ã—ã¾ã—ãŸ")

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8888, log_level="info")